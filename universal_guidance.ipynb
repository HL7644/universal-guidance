{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library for data processing\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "import itertools\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import time\n",
    "import requests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import PIL\n",
    "import scipy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import transformers\n",
    "import diffusers\n",
    "import accelerate\n",
    "import clip\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "#from torchrl.data import PrioritizedReplayBuffer, ReplayBuffer\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "base_dir='/content/drive/My Drive/Colab Notebooks/CV'\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "cpu=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clip(nn.Module):\n",
    "  def __init__(self, model):\n",
    "    super(Clip, self).__init__()\n",
    "    self.model = model\n",
    "    self.trans = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    x = TF.resize(x, (224, 224), interpolation=TF.InterpolationMode.BICUBIC)\n",
    "    x = self.trans(x)\n",
    "\n",
    "    image_features = self.model.encode_image(x)\n",
    "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "    logits_per_image = 100 * image_features @ y.t()\n",
    "    return -1 * logits_per_image\n",
    "\n",
    "  def encode(self, x):\n",
    "    x = TF.resize(x, (224, 224), interpolation=TF.InterpolationMode.BICUBIC)\n",
    "    x = self.trans(x)\n",
    "\n",
    "    image_features = self.model.encode_image(x)\n",
    "    image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalGuidancePipeline(nn.Module):\n",
    "  def __init__(self, scheduler_type):\n",
    "    super(UniversalGuidancePipeline, self).__init__()\n",
    "    assert scheduler_type in ['ddim', 'pndm'], \"Invalid Scheduler Type, should be 'ddim' or 'pndm'\"\n",
    "    if scheduler_type==\"ddim\":\n",
    "      self.scheduler=diffusers.DDIMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
    "    elif scheduler_type==\"pndm\":\n",
    "      self.scheduler=diffusers.PNDMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
    "    self.tokenizer=transformers.CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    clip_model, clip_preprocess = clip.load(\"RN50\")\n",
    "    clip_model.eval()\n",
    "    for param in clip_model.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    self.l_func = Clip(clip_model)\n",
    "    self.l_func.eval()\n",
    "    for param in self.l_func.parameters():\n",
    "      param.requires_grad = False\n",
    "    self.l_func = torch.nn.DataParallel(self.l_func).to(device)\n",
    "    \n",
    "    ldm=diffusers.StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(device)\n",
    "    #keep autoencoder parameters frozen\n",
    "    self.autoencoder=ldm.vae\n",
    "    for param in self.autoencoder.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    self.unet=ldm.unet #subject to training, f=8\n",
    "    #when is this used? for multi-modality.\n",
    "    self.text_encoder=ldm.text_encoder\n",
    "  \n",
    "  def display_intermediates(self, clean_x0, timestep, g_step):\n",
    "    print(\"Intermediate Generation for Timestep {:d}, Recurrent Step {:d}\".format(timestep, g_step+1))\n",
    "    n_imgs=clean_x0.size(0)\n",
    "    clean_x0=torch.clamp(clean_x0, min=0, max=1)\n",
    "\n",
    "    width=10\n",
    "    n_rows=n_imgs//4 if n_imgs%4==0 else n_imgs//4+1\n",
    "    n_cols=4\n",
    "    height=5*n_rows\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for idx, img in enumerate(clean_x0):\n",
    "      plt.subplot(n_rows, n_cols, idx+1)\n",
    "      plt.imshow(img.permute(1,2,0).detach().cpu().numpy())\n",
    "    plt.show()\n",
    "    return\n",
    "  \n",
    "  def normalize(self, tensor):\n",
    "    return tensor*2-1\n",
    "  \n",
    "  def unnormalize(self, tensor):\n",
    "    return (tensor+1)*0.5\n",
    "  \n",
    "  def get_x0_from_zt(self, latents, noise_pred, timestep):\n",
    "    timestep_idx=timestep-1\n",
    "    alpha_bar=self.scheduler.alphas_cumprod[timestep_idx]\n",
    "    z0_pred=(latents-torch.sqrt(1-alpha_bar)*noise_pred)/torch.sqrt(alpha_bar)\n",
    "    z0_pred=(1 / 0.18215)*z0_pred\n",
    "    clean_x0=self.autoencoder.decode(z0_pred).sample\n",
    "    clean_x0=self.unnormalize(clean_x0)\n",
    "    return clean_x0\n",
    "  \n",
    "  def get_prompt_embeddings(self, batch_size, prompt, a_prompt, n_prompt):\n",
    "    #get prompt embeddings including negative prompt for CFG.\n",
    "    prompts=[prompt+\", \"+a_prompt for _ in range(batch_size)]\n",
    "    uncond_prompts=[n_prompt for _ in range(batch_size)]\n",
    "    whole_prompts=[*prompts, *uncond_prompts]\n",
    "    with torch.no_grad():\n",
    "      tokenized=self.tokenizer(whole_prompts, return_tensors='pt', padding=True)\n",
    "      input_ids, attention_mask=tokenized.input_ids.to(device), tokenized.attention_mask.to(device)\n",
    "      prompt_embeddings=self.text_encoder(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "    return prompt_embeddings\n",
    "  \n",
    "  def predict_noise(self, input_latents, timestep, whole_prompt_embeddings, cfg_strength, no_grad=True):\n",
    "    batch_size=input_latents.size(0)\n",
    "    with torch.set_grad_enabled(not no_grad):\n",
    "      x_in = torch.cat([input_latents] * 2)\n",
    "      whole_noise = self.unet(x_in, timestep, whole_prompt_embeddings).sample\n",
    "      cond_noise=whole_noise[:batch_size]\n",
    "      uncond_noise=whole_noise[batch_size:]\n",
    "      noise_pred = uncond_noise + cfg_strength * (cond_noise - uncond_noise)\n",
    "    return noise_pred\n",
    "    \n",
    "  def forward(self, image_size, batch_size, prompt, style_image, a_prompt='best quality, extremely detailed', \n",
    "              n_prompt=\"longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality\",\n",
    "              num_inference_steps=500, cfg_strength=3, self_recurrent_steps=6, guidance_strength_coeff=6):\n",
    "    #guidance function settings\n",
    "    operation_func = None\n",
    "    other_guidance_func = None\n",
    "    criterion = self.l_func\n",
    "    other_criterion = None\n",
    "\n",
    "    intermediates=[]\n",
    "\n",
    "    #universal guidance for style transfer\n",
    "    latent_h, latent_w=image_size[0]//8, image_size[1]//8\n",
    "    whole_prompt_embeddings=self.get_prompt_embeddings(batch_size, prompt, a_prompt, n_prompt)\n",
    "    style_features=self.l_func.module.encode(style_image.unsqueeze(dim=0))\n",
    "\n",
    "    self.scheduler.set_timesteps(num_inference_steps)\n",
    "    timesteps=self.scheduler.timesteps\n",
    "    timestep_indices=timesteps-1\n",
    "\n",
    "    latents=torch.randn(batch_size, 4, latent_h, latent_w).to(device)\n",
    "    pbar=tqdm(desc=prompt, total=num_inference_steps)\n",
    "    #for each sampling step\n",
    "    for idx in range(num_inference_steps):\n",
    "      timestep=timesteps[idx]\n",
    "      timestep_idx=timestep_indices[idx]\n",
    "\n",
    "      #for each self-recurrent steps\n",
    "      for g_step in range(self_recurrent_steps):\n",
    "        torch.set_grad_enabled(True)\n",
    "        #detach input latents for gradient computation. + allow gradient computation.\n",
    "        input_latents=latents.detach().requires_grad_(True)\n",
    "\n",
    "        #CFG-applied ControlNet noise prediction\n",
    "        noise_pred=self.predict_noise(input_latents, timestep, whole_prompt_embeddings, cfg_strength, no_grad=False)\n",
    "\n",
    "        recons_image = self.get_x0_from_zt(input_latents, noise_pred, timestep)\n",
    "        intermediates.append(recons_image.detach().clone())\n",
    "        #self.display_intermediates(recons_image, timestep, g_step)\n",
    "\n",
    "        #no gradient step for Stable Diffusion version\n",
    "        #universal guidance step includes scheduler sampling.\n",
    "        selected = -1 * criterion(recons_image, style_features)\n",
    "        grad = torch.autograd.grad(selected.sum(), input_latents)[0]\n",
    "        grad = grad * guidance_strength_coeff\n",
    "\n",
    "        timestep_idx=timestep-1\n",
    "        alpha_bar=self.scheduler.alphas_cumprod[timestep_idx]\n",
    "        noise_pred = noise_pred - torch.sqrt(1-alpha_bar) * grad.detach()\n",
    "        #print(torch.mean(noise_pred), torch.std(noise_pred))\n",
    "\n",
    "        input_latents = input_latents.requires_grad_(False)\n",
    "  \n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        #updating the input latents\n",
    "        with torch.no_grad():\n",
    "          #z_(tau-1)\n",
    "          latents_prev=self.scheduler.step(noise_pred, timestep, latents).prev_sample\n",
    "\n",
    "          #actually update img\n",
    "          if idx==num_inference_steps-1:\n",
    "            alpha_bar_prev=1\n",
    "          else:\n",
    "            alpha_bar_prev=self.scheduler.alphas_cumprod[timestep_indices[idx+1]]\n",
    "          coeff=alpha_bar/alpha_bar_prev\n",
    "          eps=torch.randn_like(latents).to(device)\n",
    "\n",
    "          #should predict z_(tau) from z_(tau-1) => for this to work: should be using the DDPM whole sampling\n",
    "          latents = torch.sqrt(coeff) * latents_prev + torch.sqrt(1-coeff) * eps\n",
    "        \n",
    "      #final prediction as new latent after guidance\n",
    "      latents=latents_prev\n",
    "      pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    latents=latents.detach()\n",
    "    with torch.no_grad():\n",
    "      latents=(1 / 0.18215)*latents\n",
    "      images=self.autoencoder.decode(latents).sample\n",
    "      images=self.unnormalize(images)\n",
    "      images=torch.clamp(images, min=0, max=1)\n",
    "    return images, intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gathering style images\n",
    "preprocess=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(512),\n",
    "    torchvision.transforms.CenterCrop(512),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "picasso_url='https://render.fineartamerica.com/images/rendered/default/print/6/8/break/images/artworkimages/medium/2/pablo-picasso-painting-raq-med.jpg'\n",
    "gogh_url=\"https://th-thumbnailer.cdn-si-edu.com/GgmJe7fORYYh66TivAfZwNkCfv0=/fit-in/1600x0/filters:focal(640x640:641x641)/https://tf-cmsv2-smithsonianmag-media.s3.amazonaws.com/filer_public/3e/0b/3e0b2b4b-2b70-4309-a308-8bbf08360e94/national_gallery_of_the_faroe_islands_ai_exhibit_insprired_by_van_gogh.png\"\n",
    "pokemon_url=\"https://assets.reedpopcdn.com/-1645973608923.jpg/BROK/thumbnail/1600x900/quality/100/-1645973608923.jpg\"\n",
    "mario_url=\"https://www.pockettactics.com/wp-content/sites/pockettactics/2022/09/super-mario-maker-2-super-mario-bros-5-550x309.jpg\"\n",
    "\n",
    "response = requests.get(picasso_url)\n",
    "picasso = preprocess(PIL.Image.open(io.BytesIO(response.content))).to(device)[:3]\n",
    "response = requests.get(gogh_url)\n",
    "gogh = preprocess(PIL.Image.open(io.BytesIO(response.content))).to(device)[:3]\n",
    "response = requests.get(pokemon_url)\n",
    "pokemon = preprocess(PIL.Image.open(io.BytesIO(response.content))).to(device)[:3]\n",
    "response = requests.get(mario_url)\n",
    "mario = preprocess(PIL.Image.open(io.BytesIO(response.content))).to(device)[:3]\n",
    "\n",
    "#display example style images\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(picasso.permute(1,2,0).cpu().numpy())\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(gogh.permute(1,2,0).cpu().numpy())\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(pokemon.permute(1,2,0).cpu().numpy())\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(mario.permute(1,2,0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=UniversalGuidancePipeline('ddim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gen=2\n",
    "prompt=\"a hot air balloon cruising over a local village at daytime\"\n",
    "style_image=pokemon\n",
    "\n",
    "styled_images, intermediates=pipeline(image_size=(512,512), batch_size=n_gen, prompt=prompt, style_image=style_image, num_inference_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Styled Images\")\n",
    "plt.figure(figsize=(15,5))\n",
    "for idx, image in enumerate(styled_images):\n",
    "  plt.subplot(1, n_gen, idx+1)\n",
    "  plt.imshow(image.permute(1,2,0).detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,25))\n",
    "len_interm=len(intermediates)\n",
    "show_every=10\n",
    "n_imgs=len_interm//show_every\n",
    "n_row=int(np.sqrt(n_imgs))+1\n",
    "for idx in range(n_imgs):\n",
    "  plt.subplot(n_row,n_row,idx+1)\n",
    "  img_idx=show_every*idx\n",
    "  img=torch.clamp(intermediates[img_idx].squeeze(dim=0), min=0, max=1)\n",
    "  plt.imshow(img.permute(1,2,0).detach().cpu().numpy())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "641a7458bfae2bc959d7f867e9e3882167acabe29543290f7c5231fa0d54378e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
